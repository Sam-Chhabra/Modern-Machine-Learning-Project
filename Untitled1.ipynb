{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/gnsJ2YNYELoQX+4ZiEcn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sam-Chhabra/Modern-Machine-Learning-Project/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1is0IJVcDdE4"
      },
      "outputs": [],
      "source": [
        "# LightGBM is usually available on Colab; install if missing.\n",
        "!pip -q install lightgbm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/MML_Project\")\n",
        "RECEIVALS_CSV = DATA_DIR / \"receivals.csv\"\n",
        "PO_CSV        = DATA_DIR / \"purchase_orders.csv\"\n",
        "SAMPLE_CSV    = DATA_DIR / \"sample_submission.csv\"\n",
        "MAP_CSV       = DATA_DIR / \"prediction_mapping.csv\"  # may or may not be needed\n",
        "OUT_CSV       = DATA_DIR / \"final_submission.csv\"    # required by your project folder\n",
        "\n",
        "def read_csv_auto(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_csv(path, sep=None, engine=\"python\")\n",
        "\n",
        "# --- Load receivals ---\n",
        "receivals = read_csv_auto(RECEIVALS_CSV)\n",
        "\n",
        "# Parse timestamps to tz-naive (UTC→naive)\n",
        "receivals[\"date_arrival\"] = (\n",
        "    pd.to_datetime(receivals[\"date_arrival\"], errors=\"coerce\", utc=True)\n",
        "      .dt.tz_localize(None)\n",
        ")\n",
        "\n",
        "# Basic checks\n",
        "assert {\"rm_id\",\"net_weight\",\"date_arrival\"}.issubset(receivals.columns), \\\n",
        "    f\"receivals missing required columns. Found: {receivals.columns.tolist()}\"\n",
        "\n",
        "# --- Load purchase_orders if present ---\n",
        "purchase_orders = None\n",
        "if PO_CSV.exists():\n",
        "    po = read_csv_auto(PO_CSV)\n",
        "    # Strip whitespace from object columns\n",
        "    for c in po.select_dtypes(include=\"object\").columns:\n",
        "        po[c] = po[c].astype(str).str.strip()\n",
        "    # Parse datetimes with utc then drop tz\n",
        "    if \"modified_date_time\" in po.columns:\n",
        "        po[\"modified_date_time\"] = (\n",
        "            pd.to_datetime(po[\"modified_date_time\"], errors=\"coerce\", utc=True)\n",
        "              .dt.tz_localize(None)\n",
        "        )\n",
        "    if \"delivery_date\" in po.columns:\n",
        "        po[\"delivery_date\"] = (\n",
        "            pd.to_datetime(po[\"delivery_date\"], errors=\"coerce\", utc=True)\n",
        "              .dt.tz_localize(None)\n",
        "        )\n",
        "    if \"quantity\" in po.columns:\n",
        "        po[\"quantity\"] = pd.to_numeric(po[\"quantity\"], errors=\"coerce\")\n",
        "    purchase_orders = po\n",
        "\n",
        "print(\"Loaded:\",\n",
        "      f\"\\n- receivals: {receivals.shape}\",\n",
        "      f\"\\n- purchase_orders: {None if purchase_orders is None else purchase_orders.shape}\",\n",
        "      f\"\\nFiles in dir: {sorted([p.name for p in DATA_DIR.iterdir()])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZS_3vmVDpDl",
        "outputId": "b854582d-71b5-4c8a-f553-c2b28dcf0a8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded: \n",
            "- receivals: (122590, 10) \n",
            "- purchase_orders: (33171, 12) \n",
            "Files in dir: ['Cartel1.xlsx', 'final_submission.csv', 'materials.csv', 'prediction_mapping.csv', 'purchase_orders.csv', 'receivals.csv', 'sample_submission.csv', 'transportation.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jan_may_window(df: pd.DataFrame, year: int) -> pd.DataFrame:\n",
        "    start = pd.Timestamp(f\"{year}-01-01\")\n",
        "    end   = pd.Timestamp(f\"{year}-05-31\")\n",
        "    return df.loc[df[\"date_arrival\"].between(start, end, inclusive=\"both\")]\n",
        "\n",
        "def cumulative_jan_may_by_rm(df: pd.DataFrame, year: int, col_out: str) -> pd.DataFrame:\n",
        "    \"\"\"Sum net_weight in Jan–May (inclusive) for 'year', grouped by rm_id.\"\"\"\n",
        "    sub = jan_may_window(df, year)\n",
        "    out = (sub.groupby(\"rm_id\", as_index=False)[\"net_weight\"]\n",
        "              .sum()\n",
        "              .rename(columns={\"net_weight\": col_out}))\n",
        "    return out\n",
        "\n",
        "def build_lag_frame(receivals: pd.DataFrame, Y: int, max_lag: int = 3) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For target year Y, build:\n",
        "    rm_id | cum_lag1 | cum_lag2 | cum_lag3 | y_target | year\n",
        "    y_target = cumulative Jan–May of Y\n",
        "    cum_lagK = cumulative Jan–May of (Y-K)\n",
        "    \"\"\"\n",
        "    tgt = cumulative_jan_may_by_rm(receivals, Y, f\"cum_{Y}_to_0531\")\n",
        "    if tgt.empty:\n",
        "        return pd.DataFrame(columns=[\"rm_id\",\"cum_lag1\",\"cum_lag2\",\"cum_lag3\",\"y_target\",\"year\"])\n",
        "\n",
        "    tgt = tgt.set_index(\"rm_id\")\n",
        "    X = pd.DataFrame(index=tgt.index)\n",
        "    rename_map = {}\n",
        "\n",
        "    for k in range(1, max_lag + 1):\n",
        "        lag_year = Y - k\n",
        "        lag_df = cumulative_jan_may_by_rm(receivals, lag_year, f\"cum_{lag_year}_to_0531\")\n",
        "        if not lag_df.empty:\n",
        "            X = X.join(lag_df.set_index(\"rm_id\"), how=\"left\")\n",
        "            rename_map[f\"cum_{lag_year}_to_0531\"] = f\"cum_lag{k}\"\n",
        "\n",
        "    X = X.join(tgt, how=\"left\").rename(columns=rename_map)\n",
        "    X = X.rename(columns={f\"cum_{Y}_to_0531\": \"y_target\"})\n",
        "    X[\"year\"] = Y\n",
        "    return X.reset_index()"
      ],
      "metadata": {
        "id": "hcqyGgm2D1lP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add year/day columns\n",
        "receivals = receivals.copy()\n",
        "receivals[\"day\"]  = receivals[\"date_arrival\"].dt.normalize()\n",
        "receivals[\"year\"] = receivals[\"day\"].dt.year\n",
        "\n",
        "years_all  = sorted(receivals[\"year\"].dropna().unique())\n",
        "years_hist = [y for y in years_all if 2005 <= y <= 2024]  # keep a sensible range\n",
        "\n",
        "rows = []\n",
        "for Y in years_hist:\n",
        "    lf = build_lag_frame(receivals, Y, max_lag=3)\n",
        "    if not lf.empty:\n",
        "        rows.append(lf)\n",
        "\n",
        "train_df = (pd.concat(rows, ignore_index=True)\n",
        "            if rows else pd.DataFrame(columns=[\"rm_id\",\"cum_lag1\",\"cum_lag2\",\"cum_lag3\",\"y_target\",\"year\"]))\n",
        "\n",
        "# Feature engineering\n",
        "for c in [\"cum_lag1\",\"cum_lag2\",\"cum_lag3\"]:\n",
        "    if c not in train_df.columns:\n",
        "        train_df[c] = np.nan\n",
        "\n",
        "train_df[\"lag_mean_12\"]  = train_df[[\"cum_lag1\",\"cum_lag2\"]].mean(axis=1, skipna=True)\n",
        "train_df[\"lag_mean_123\"] = train_df[[\"cum_lag1\",\"cum_lag2\",\"cum_lag3\"]].mean(axis=1, skipna=True)\n",
        "\n",
        "# Keep rows with target and at least 1 feature\n",
        "feat_cols = [\"cum_lag1\",\"cum_lag2\",\"cum_lag3\",\"lag_mean_12\",\"lag_mean_123\"]\n",
        "train_df  = train_df.dropna(subset=[\"y_target\"])\n",
        "train_df  = train_df.dropna(subset=feat_cols, how=\"all\")\n",
        "\n",
        "X_all     = train_df[feat_cols].fillna(0.0)\n",
        "y_all     = train_df[\"y_target\"].astype(float)\n",
        "\n",
        "print(\"Training table:\", train_df.shape)\n",
        "print(\"Years in data:\", sorted(train_df['year'].unique())[:5], \"...\", sorted(train_df['year'].unique())[-5:])\n",
        "print(\"Feature columns:\", feat_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBIaOh1ED3EC",
        "outputId": "ba9aa53a-6cc8-496e-bbc0-a27b726bb5be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training table: (414, 8)\n",
            "Years in data: [np.int32(2006), np.int32(2007), np.int32(2008), np.int32(2009), np.int32(2010)] ... [np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024)]\n",
            "Feature columns: ['cum_lag1', 'cum_lag2', 'cum_lag3', 'lag_mean_12', 'lag_mean_123']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5a — Diagnose feature signal vs target (optional)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "diag = train_df[[\"rm_id\",\"year\",\"y_target\"] + feat_cols].copy()\n",
        "print(\"Feature std (train):\")\n",
        "print(diag[feat_cols].std().sort_values())\n",
        "\n",
        "print(\"\\nPercent of zeros per feature:\")\n",
        "print((diag[feat_cols]==0).mean().sort_values())\n",
        "\n",
        "print(\"\\nCorr with target (Pearson):\")\n",
        "print(diag[feat_cols + [\"y_target\"]].corr(numeric_only=True)[\"y_target\"].sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7OTLeuyGSs-",
        "outputId": "4ccf77cd-60be-4898-d5f8-6a39dbec2d5a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature std (train):\n",
            "lag_mean_123    2.939555e+06\n",
            "lag_mean_12     2.994359e+06\n",
            "cum_lag1        3.118527e+06\n",
            "cum_lag2        3.221352e+06\n",
            "cum_lag3        3.342229e+06\n",
            "dtype: float64\n",
            "\n",
            "Percent of zeros per feature:\n",
            "cum_lag1        0.0\n",
            "cum_lag2        0.0\n",
            "cum_lag3        0.0\n",
            "lag_mean_12     0.0\n",
            "lag_mean_123    0.0\n",
            "dtype: float64\n",
            "\n",
            "Corr with target (Pearson):\n",
            "y_target        1.000000\n",
            "cum_lag1        0.953124\n",
            "lag_mean_12     0.943364\n",
            "lag_mean_123    0.938486\n",
            "cum_lag2        0.921018\n",
            "cum_lag3        0.909841\n",
            "Name: y_target, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5b — Train GradientBoostingRegressor (quantile) on log-scale\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "unique_years = sorted(train_df[\"year\"].unique())\n",
        "valid_years = unique_years[-2:] if len(unique_years) >= 4 else unique_years[-1:]\n",
        "\n",
        "train_mask  = ~train_df[\"year\"].isin(valid_years)\n",
        "valid_mask  =  train_df[\"year\"].isin(valid_years)\n",
        "\n",
        "X_train_raw, y_train_raw = X_all.loc[train_mask], y_all.loc[train_mask]\n",
        "X_valid_raw, y_valid_raw = X_all.loc[valid_mask], y_all.loc[valid_mask]\n",
        "\n",
        "# log1p transforms\n",
        "X_train = np.log1p(X_train_raw)\n",
        "X_valid = np.log1p(X_valid_raw)\n",
        "y_train = np.log1p(y_train_raw.clip(lower=0))\n",
        "y_valid = np.log1p(y_valid_raw.clip(lower=0))\n",
        "\n",
        "print(f\"Train samples: {len(X_train)}, Validation samples: {len(X_valid)}, Valid years: {valid_years}\")\n",
        "\n",
        "gbr = GradientBoostingRegressor(\n",
        "    loss=\"quantile\",\n",
        "    alpha=0.2,           # tau = 0.2\n",
        "    n_estimators=1200,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=3,\n",
        "    subsample=0.9,\n",
        "    random_state=42\n",
        ")\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# validate on original scale\n",
        "pred_valid_log = gbr.predict(X_valid)\n",
        "pred_valid     = np.expm1(pred_valid_log).clip(min=0)\n",
        "\n",
        "def quantile_loss(y_true, y_pred, tau=0.2):\n",
        "    diff = y_true - y_pred\n",
        "    return np.mean(np.clip(diff, 0, None)*tau + np.clip(-diff, 0, None)*(1-tau))\n",
        "\n",
        "print(f\"Validation quantile loss (tau=0.2): {quantile_loss(y_valid_raw.values, pred_valid, 0.2):.2f} kg\")\n",
        "\n",
        "# IMPORTANT for later cells (6–8):\n",
        "# Replace `model` with `gbr` and keep using the same log1p/expm1 pattern:\n",
        "#   pred_2025  = np.expm1(gbr.predict(np.log1p(X_2025_raw))).clip(min=0)\n",
        "model = gbr  # so your Cell 6 can stay the same except calling model.predict(...)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhsBIkSRD55N",
        "outputId": "abcfcbbe-6135-431b-9627-1f926f2d7da1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 352, Validation samples: 62, Valid years: [np.int32(2023), np.int32(2024)]\n",
            "Validation quantile loss (tau=0.2): 212682.01 kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6 — Build 2025 features & raw predictions (log→orig)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def cumulative_jan_may_by_rm(df: pd.DataFrame, year: int, col_out: str) -> pd.DataFrame:\n",
        "    start = pd.Timestamp(f\"{year}-01-01\")\n",
        "    end   = pd.Timestamp(f\"{year}-05-31\")\n",
        "    sub = df.loc[df[\"date_arrival\"].between(start, end, inclusive=\"both\")]\n",
        "    out = (sub.groupby(\"rm_id\", as_index=False)[\"net_weight\"]\n",
        "              .sum()\n",
        "              .rename(columns={\"net_weight\": col_out}))\n",
        "    return out\n",
        "\n",
        "def lag_frame_for_prediction(receivals: pd.DataFrame, target_year: int, max_lag=3) -> pd.DataFrame:\n",
        "    pred = pd.DataFrame({\"rm_id\": receivals[\"rm_id\"].dropna().unique()})\n",
        "    for k in range(1, max_lag+1):\n",
        "        y = target_year - k\n",
        "        col = f\"cum_lag{k}\"\n",
        "        lag_df = cumulative_jan_may_by_rm(receivals, y, f\"cum_{y}_to_0531\")\n",
        "        pred = pred.merge(lag_df.rename(columns={f\"cum_{y}_to_0531\": col}),\n",
        "                          on=\"rm_id\", how=\"left\")\n",
        "    # derived features\n",
        "    for c in [\"cum_lag1\",\"cum_lag2\",\"cum_lag3\"]:\n",
        "        if c not in pred.columns: pred[c] = np.nan\n",
        "    pred[\"lag_mean_12\"]  = pred[[\"cum_lag1\",\"cum_lag2\"]].mean(axis=1, skipna=True)\n",
        "    pred[\"lag_mean_123\"] = pred[[\"cum_lag1\",\"cum_lag2\",\"cum_lag3\"]].mean(axis=1, skipna=True)\n",
        "    return pred\n",
        "\n",
        "# Build 2025 feature table\n",
        "features_2025 = lag_frame_for_prediction(receivals, 2025, max_lag=3)\n",
        "\n",
        "# Use the same feature order as in training (feat_cols defined earlier)\n",
        "feature_cols = [\"cum_lag1\",\"cum_lag2\",\"cum_lag3\",\"lag_mean_12\",\"lag_mean_123\"]\n",
        "X_2025_raw = features_2025[feature_cols].fillna(0.0)\n",
        "\n",
        "# Predict (log1p in, expm1 out) with the model from Cell 5b (GBR) or Cell 5A (LGBM)\n",
        "X_2025_log = np.log1p(X_2025_raw)\n",
        "pred_2025  = np.expm1(model.predict(X_2025_log)).clip(min=0)\n",
        "\n",
        "# Collect predictions\n",
        "pred_df = features_2025[[\"rm_id\"]].copy()\n",
        "pred_df[\"pred_cum_kg_raw\"] = pred_2025\n",
        "\n",
        "print(\"2025 predictions ready. Rows:\", len(pred_df), \"Unique rm_id:\", pred_df[\"rm_id\"].nunique())\n",
        "pred_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "PFTCvnuEHeF7",
        "outputId": "360521b5-15d1-4722-b662-371b4fe680d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025 predictions ready. Rows: 203 Unique rm_id: 203\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rm_id  pred_cum_kg_raw\n",
              "0  365.0      4831.124558\n",
              "1  379.0      4831.124558\n",
              "2  389.0      4831.124558\n",
              "3  369.0      4831.124558\n",
              "4  366.0      4831.124558"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-651f11e9-9717-4974-9161-72fadee9989f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rm_id</th>\n",
              "      <th>pred_cum_kg_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>365.0</td>\n",
              "      <td>4831.124558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>379.0</td>\n",
              "      <td>4831.124558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>389.0</td>\n",
              "      <td>4831.124558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>369.0</td>\n",
              "      <td>4831.124558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>366.0</td>\n",
              "      <td>4831.124558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-651f11e9-9717-4974-9161-72fadee9989f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-651f11e9-9717-4974-9161-72fadee9989f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-651f11e9-9717-4974-9161-72fadee9989f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-74703e0a-fbef-4656-86c9-125b36f9a7cc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-74703e0a-fbef-4656-86c9-125b36f9a7cc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-74703e0a-fbef-4656-86c9-125b36f9a7cc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pred_df",
              "summary": "{\n  \"name\": \"pred_df\",\n  \"rows\": 203,\n  \"fields\": [\n    {\n      \"column\": \"rm_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1127.3584931261248,\n        \"min\": 342.0,\n        \"max\": 4501.0,\n        \"num_unique_values\": 203,\n        \"samples\": [\n          381.0,\n          347.0,\n          2323.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_cum_kg_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 710546.8434348573,\n        \"min\": 1133.8850666439034,\n        \"max\": 6219999.707623693,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          4831.124558138975,\n          3032558.9620954394,\n          6219999.707623693\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7 — PO-aware cap (2025-only, kg-only, conservative)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure dtypes for consistent joins\n",
        "pred_df[\"rm_id\"] = pred_df[\"rm_id\"].astype(\"Int64\")\n",
        "if \"rm_id\" in receivals.columns:\n",
        "    receivals[\"rm_id\"] = receivals[\"rm_id\"].astype(\"Int64\")\n",
        "\n",
        "start_2025 = pd.Timestamp(\"2025-01-01\")\n",
        "end_date   = pd.Timestamp(\"2025-05-31\")\n",
        "\n",
        "po_cap_rm = pd.DataFrame(columns=[\"rm_id\",\"po_cap_kg\"])\n",
        "\n",
        "if (purchase_orders is not None) and \\\n",
        "   (\"delivery_date\" in purchase_orders.columns) and \\\n",
        "   (\"quantity\" in purchase_orders.columns):\n",
        "\n",
        "    po_last = purchase_orders.copy()\n",
        "\n",
        "    # Clean types\n",
        "    if \"purchase_order_id\" in po_last.columns:\n",
        "        po_last[\"purchase_order_id\"] = pd.to_numeric(po_last[\"purchase_order_id\"], errors=\"coerce\")\n",
        "    if \"purchase_order_item_no\" in po_last.columns:\n",
        "        po_last[\"purchase_order_item_no\"] = pd.to_numeric(po_last[\"purchase_order_item_no\"], errors=\"coerce\")\n",
        "    if \"product_id\" in po_last.columns:\n",
        "        po_last[\"product_id\"] = pd.to_numeric(po_last[\"product_id\"], errors=\"coerce\")\n",
        "    po_last[\"delivery_date\"] = pd.to_datetime(po_last[\"delivery_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "    po_last[\"quantity\"] = pd.to_numeric(po_last[\"quantity\"], errors=\"coerce\")\n",
        "\n",
        "    # Keep latest revision per item (if keys exist)\n",
        "    item_keys = [k for k in [\"purchase_order_id\",\"purchase_order_item_no\"] if k in po_last.columns]\n",
        "    if item_keys:\n",
        "        sort_cols = item_keys + ([\"modified_date_time\"] if \"modified_date_time\" in po_last.columns else [])\n",
        "        po_sorted = po_last.sort_values(sort_cols)\n",
        "        po_last   = po_sorted.drop_duplicates(subset=item_keys, keep=\"last\")\n",
        "\n",
        "    # --- STRICT FILTERS for 2025-only cap ---\n",
        "    mask = po_last[\"delivery_date\"].between(start_2025, end_date, inclusive=\"both\")\n",
        "    if \"unit\" in po_last.columns:\n",
        "        mask &= po_last[\"unit\"].astype(str).str.lower().str.strip().eq(\"kg\")\n",
        "    if \"status\" in po_last.columns:\n",
        "        # keep non-deleted/cancelled; adjust if your data uses different labels\n",
        "        status = po_last[\"status\"].astype(str).str.lower().str.strip()\n",
        "        mask &= ~status.isin({\"deleted\", \"cancelled\", \"canceled\"})\n",
        "    po_due = po_last.loc[mask].copy()\n",
        "\n",
        "    # Aggregate by product, then map to rm_id\n",
        "    if not po_due.empty and \"product_id\" in po_due.columns:\n",
        "        po_due_sum = (po_due.groupby(\"product_id\", as_index=False)[\"quantity\"]\n",
        "                            .sum()\n",
        "                            .rename(columns={\"quantity\":\"ordered_due_qty\"}))\n",
        "\n",
        "        if \"product_id\" in receivals.columns:\n",
        "            rm_prod = receivals[[\"rm_id\",\"product_id\"]].dropna().drop_duplicates()\n",
        "            rm_prod[\"product_id\"] = pd.to_numeric(rm_prod[\"product_id\"], errors=\"coerce\")\n",
        "            po_cap_rm = (rm_prod.merge(po_due_sum, on=\"product_id\", how=\"left\")\n",
        "                               .groupby(\"rm_id\", as_index=False)[\"ordered_due_qty\"].sum())\n",
        "        else:\n",
        "            po_cap_rm = pd.DataFrame(columns=[\"rm_id\",\"ordered_due_qty\"])\n",
        "\n",
        "# Apply conservative safety factor\n",
        "if not po_cap_rm.empty:\n",
        "    po_cap_rm[\"po_cap_kg\"] = po_cap_rm[\"ordered_due_qty\"].fillna(0.0) * 0.9\n",
        "else:\n",
        "    po_cap_rm[\"po_cap_kg\"] = pd.Series(dtype=float)\n",
        "\n",
        "po_cap_rm[\"rm_id\"] = po_cap_rm[\"rm_id\"].astype(\"Int64\")\n",
        "\n",
        "# Count how many due PO lines contributed to each rm_id's cap\n",
        "if \"product_id\" in purchase_orders.columns and not po_cap_rm.empty:\n",
        "    # Build counts by product, then map to rm_id\n",
        "    po_due_cnt_by_prod = (\n",
        "        po_due.groupby(\"product_id\", as_index=False)\n",
        "              .size()\n",
        "              .rename(columns={\"size\": \"po_due_count\"})\n",
        "    )\n",
        "    rm_prod = receivals[[\"rm_id\",\"product_id\"]].dropna().drop_duplicates()\n",
        "    rm_prod[\"product_id\"] = pd.to_numeric(rm_prod[\"product_id\"], errors=\"coerce\")\n",
        "    po_cnt_rm = (rm_prod.merge(po_due_cnt_by_prod, on=\"product_id\", how=\"left\")\n",
        "                        .groupby(\"rm_id\", as_index=False)[\"po_due_count\"].sum())\n",
        "else:\n",
        "    po_cnt_rm = pd.DataFrame({\"rm_id\": pred_df[\"rm_id\"], \"po_due_count\": 0})\n",
        "\n",
        "# Merge counts and cap\n",
        "pred_final = (pred_df\n",
        "              .merge(po_cap_rm[[\"rm_id\",\"po_cap_kg\"]], on=\"rm_id\", how=\"left\")\n",
        "              .merge(po_cnt_rm, on=\"rm_id\", how=\"left\"))\n",
        "pred_final[\"po_due_count\"] = pred_final[\"po_due_count\"].fillna(0).astype(int)\n",
        "\n",
        "# Apply cap ONLY if there is at least one due PO line for that rm_id\n",
        "pred_final[\"prediction_kg\"] = np.where(\n",
        "    (pred_final[\"po_due_count\"] > 0) & pred_final[\"po_cap_kg\"].notna(),\n",
        "    np.minimum(pred_final[\"pred_cum_kg_raw\"], pred_final[\"po_cap_kg\"]),\n",
        "    pred_final[\"pred_cum_kg_raw\"]\n",
        ")\n",
        "\n",
        "# Diagnostics\n",
        "caps_available = (pred_final[\"po_due_count\"] > 0).sum()\n",
        "caps_binding   = ((pred_final[\"po_due_count\"] > 0) &\n",
        "                  (pred_final[\"po_cap_kg\"] < pred_final[\"pred_cum_kg_raw\"])).sum()\n",
        "print(f\"Caps available for {caps_available}/{len(pred_final)} rm_id; binding for {caps_binding} rm_id.\")\n",
        "\n",
        "print(\"\\nPreview (rm_id, raw, cap, count, final):\")\n",
        "print(pred_final[[\"rm_id\",\"pred_cum_kg_raw\",\"po_cap_kg\",\"po_due_count\",\"prediction_kg\"]]\n",
        "      .sort_values(\"rm_id\")\n",
        "      .head(12))\n",
        "\n",
        "# Keep only the columns needed for submission\n",
        "pred_final = pred_final[[\"rm_id\",\"prediction_kg\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VmTIy0gIEyu",
        "outputId": "3933a61d-0c7d-455a-9f0b-e3044c58b7ac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caps available for 92/203 rm_id; binding for 8 rm_id.\n",
            "\n",
            "Preview (rm_id, raw, cap, count, final):\n",
            "     rm_id  pred_cum_kg_raw   po_cap_kg  po_due_count  prediction_kg\n",
            "12     342      4831.124558         0.0             0    4831.124558\n",
            "29     343      4831.124558  10071000.0            27    4831.124558\n",
            "20     345      4831.124558  10071000.0            27    4831.124558\n",
            "13     346      4831.124558    972000.0            17    4831.124558\n",
            "9      347      4831.124558  10071000.0            27    4831.124558\n",
            "21     348      4831.124558  10071000.0            27    4831.124558\n",
            "24     353      4831.124558  10071000.0            27    4831.124558\n",
            "16     354      4831.124558         0.0             0    4831.124558\n",
            "135    355      4831.124558         0.0             0    4831.124558\n",
            "10     357      4831.124558         0.0             0    4831.124558\n",
            "22     358      4831.124558         0.0             0    4831.124558\n",
            "18     360      4831.124558    747000.0            14    4831.124558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8 — Build submission and save to final_submission.csv (handles ['ID','predicted_weight'])\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "try:\n",
        "    DATA_DIR\n",
        "except NameError:\n",
        "    DATA_DIR = Path(\"/content/drive/MyDrive/MML_Project\")\n",
        "\n",
        "SAMPLE_CSV = DATA_DIR / \"sample_submission.csv\"\n",
        "MAP_CSV    = DATA_DIR / \"prediction_mapping.csv\"\n",
        "OUT_CSV    = DATA_DIR / \"final_submission.csv\"\n",
        "\n",
        "# Preconditions\n",
        "assert 'pred_final' in globals(), \"pred_final not found. Run Cells 6/7 first.\"\n",
        "assert {\"rm_id\",\"prediction_kg\"}.issubset(pred_final.columns), \"pred_final must have ['rm_id','prediction_kg']\"\n",
        "\n",
        "# 1) Load sample\n",
        "sample = pd.read_csv(SAMPLE_CSV)\n",
        "\n",
        "# 2) Detect target column (extend list to include 'predicted_weight')\n",
        "target_candidates = {\"prediction\",\"predicted_weight\",\"target\",\"value\",\"label\"}\n",
        "lower_map = {c.lower(): c for c in sample.columns}\n",
        "target_lower = next((t for t in target_candidates if t in lower_map), None)\n",
        "if target_lower is None:\n",
        "    raise ValueError(f\"Cannot infer target column from sample: {sample.columns.tolist()}\")\n",
        "target_col = lower_map[target_lower]  # original casing (e.g., 'predicted_weight')\n",
        "\n",
        "# 3) If sample already has rm_id, merge directly; otherwise use prediction_mapping.csv\n",
        "#    We will try a robust, case-insensitive join key match between sample and mapping.\n",
        "pred_final = pred_final.copy()\n",
        "pred_final[\"rm_id\"] = pd.to_numeric(pred_final[\"rm_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "if \"rm_id\" in sample.columns:\n",
        "    # Align dtype and merge\n",
        "    sample[\"rm_id\"] = pd.to_numeric(sample[\"rm_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    out = sample.merge(\n",
        "        pred_final.rename(columns={\"prediction_kg\": target_col}),\n",
        "        on=\"rm_id\", how=\"left\"\n",
        "    )\n",
        "else:\n",
        "    # Need mapping file to translate sample key -> rm_id\n",
        "    if not MAP_CSV.exists():\n",
        "        raise FileNotFoundError(\n",
        "            \"prediction_mapping.csv not found, and sample_submission has no 'rm_id' column.\"\n",
        "        )\n",
        "    mapping = pd.read_csv(MAP_CSV)\n",
        "\n",
        "    # Build case-insensitive column dicts\n",
        "    map_lower_to_orig = {c.lower(): c for c in mapping.columns}\n",
        "    samp_lower_to_orig = {c.lower(): c for c in sample.columns}\n",
        "\n",
        "    # Find a shared key (not 'rm_id'), case-insensitive (e.g., 'ID' in sample ↔ 'id' in mapping)\n",
        "    shared_keys = [s for s in samp_lower_to_orig.keys()\n",
        "                   if s != \"rm_id\" and s in map_lower_to_orig and map_lower_to_orig[s] != \"rm_id\"]\n",
        "    if not shared_keys:\n",
        "        raise ValueError(\n",
        "            f\"No common key between mapping {mapping.columns.tolist()} and sample {sample.columns.tolist()}\"\n",
        "        )\n",
        "    shared_key_lower = shared_keys[0]\n",
        "    sample_key = samp_lower_to_orig[shared_key_lower]\n",
        "    mapping_key = map_lower_to_orig[shared_key_lower]\n",
        "\n",
        "    # Align dtypes for the shared key and rm_id\n",
        "    mapping[\"rm_id\"] = pd.to_numeric(mapping[\"rm_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    sample[sample_key] = pd.to_numeric(sample[sample_key], errors=\"coerce\")\n",
        "    mapping[mapping_key] = pd.to_numeric(mapping[mapping_key], errors=\"coerce\")\n",
        "\n",
        "    # Merge: sample ← mapping (adds rm_id) ← predictions\n",
        "    out = (sample.merge(mapping[[mapping_key, \"rm_id\"]], left_on=sample_key, right_on=mapping_key, how=\"left\")\n",
        "                 .merge(pred_final.rename(columns={\"prediction_kg\": target_col}), on=\"rm_id\", how=\"left\"))\n",
        "\n",
        "    # Keep only original sample columns + target\n",
        "    keep_cols = sample.columns.tolist()\n",
        "    if target_col not in keep_cols:\n",
        "        keep_cols.append(target_col)\n",
        "    out = out[keep_cols]\n",
        "\n",
        "# 4) Fill missing predictions conservatively with 0 and save\n",
        "out[target_col] = pd.to_numeric(out[target_col], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "out.to_csv(OUT_CSV, index=False)\n",
        "print(f\"Saved submission → {OUT_CSV}\")\n",
        "print(\"Rows:\", len(out), \"| NaNs in target:\", out[target_col].isna().sum())\n",
        "print(out.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "0HlPZOwmMYO7",
        "outputId": "756025e4-80c6-465b-cadf-90e7c617e0c4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['predicted_weight'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3375563913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeep_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mkeep_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# 4) Fill missing predictions conservatively with 0 and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['predicted_weight'] not in index\""
          ]
        }
      ]
    }
  ]
}